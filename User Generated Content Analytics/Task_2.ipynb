{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UGCA Group Assignment -2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Members\n",
    "1. Chetna Singhal (cs57926)\n",
    "2. David Kinman\n",
    "3. Prajval Gupta\n",
    "4. Subhayu Chakravarty\n",
    "5. Whitt Hyde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajvalgupta/.local/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['copy']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "#Import statements\n",
    "import os\n",
    "import re\n",
    "# import tweepy as tw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from time import sleep\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from nltk import ngrams, FreqDist\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, webtext\n",
    "from sklearn import manifold\n",
    "import nltk.data\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as sentiment\n",
    "# from textblob import TextBlob\n",
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART A - Collecting tweets using Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('secrets.txt', 'r') as file:\n",
    "    data = file.read().split('\\n')\n",
    "    \n",
    "consumer_key = data[0]\n",
    "consumer_secret = data[1]\n",
    "access_key = data[2]\n",
    "access_secret = data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tw.API(auth,wait_on_rate_limit=True,\n",
    "    wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_tweets(query, num=0):\n",
    "    num = 3000 if num > 3000 else num\n",
    "    max_num_per_call = 100\n",
    "\n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tw.API(auth)\n",
    "    \n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []    \n",
    "    \n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    curr_count = max_num_per_call if num > max_num_per_call else num\n",
    "    num -= curr_count\n",
    "\n",
    "    new_tweets = api.search(q=query, count=curr_count)\n",
    "    \n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    \n",
    "    print(f\"{len(alltweets)} tweets downloaded so far\")\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while num > 0:\n",
    "        print(f\"Getting tweets before {oldest}\")\n",
    "        \n",
    "        curr_count = max_num_per_call if num > max_num_per_call else num\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.search(q=query, count=curr_count, max_id=oldest)\n",
    "        num -= curr_count\n",
    "        \n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "        \n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        \n",
    "        print(f\"{len(alltweets)} tweets downloaded so far\")\n",
    "    \n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv    \n",
    "    outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\"), tweet.user.location] for tweet in alltweets]\n",
    "    df = pd.DataFrame(outtweets, columns=[\"id\", \"created_at\", \"text\", \"location\"])\n",
    "    #df.to_csv(f\"query_{query}.csv\", index=False)\n",
    "    #print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in the search query\n",
    "new_search = \"#2020Election\" + \" -filter:retweets\"\n",
    "tweets1 = get_query_tweets(new_search, 2000)\n",
    "tweets2 = get_query_tweets(\"2020 U.S. election\", 1000)\n",
    "tweets3 = get_query_tweets(\"2020 presidential election\", 1000)\n",
    "new_search = \"#election2020\" + \" -filter:retweets\"\n",
    "tweets4 = get_query_tweets(new_search, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets1.shape,tweets2.shape,tweets3.shape,tweets4.shape)\n",
    "tweets=tweets1.append(tweets2, ignore_index=True)\n",
    "tweets=tweets.append(tweets3, ignore_index=True)\n",
    "tweets=tweets.append(tweets4, ignore_index=True)\n",
    "print(tweets.shape)\n",
    "tweets.to_csv(r\"tweets.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuation characters from each tweet and convert text to lowercase for efficient frequency counting\n",
    "# punctuation includes !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "def remove_punctuations(item):\n",
    "    for p in punctuation:\n",
    "        item = item.strip().replace(p,'')\n",
    "    return item\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(s):\n",
    "    return [w for w in s if not w in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Tweet Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'No reparations no vote! No BLACK agenda no V...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Thoughts? San Francisco ::   Adam Hattersley...</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'@TheJuanWilliams Why why why are you here???...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'#BernieSanders = #Socialism which leads to #...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'@AOC should re-think her endorsement of inde...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Tweet Length\n",
       "0  b'No reparations no vote! No BLACK agenda no V...            72\n",
       "1  b'Thoughts? San Francisco ::   Adam Hattersley...           153\n",
       "2  b'@TheJuanWilliams Why why why are you here???...            85\n",
       "3  b'#BernieSanders = #Socialism which leads to #...           155\n",
       "4  b'@AOC should re-think her endorsement of inde...           154"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load user comments into a dataframe\n",
    "df = pd.read_csv(\"tweets.csv\", usecols=[\"text\"])\n",
    "df.columns = [\"Tweets\"]\n",
    "\n",
    "# Remove newline from each post & drop rows with null values\n",
    "df = df.replace('\\n','', regex=True)\n",
    "df = df.dropna()\n",
    "\n",
    "df[\"Tweet Length\"]= df[\"Tweets\"].str.len() \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "def tweet_cleaner1(text):\n",
    "    \n",
    "    # HTML Decoding\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    souped = souped.replace(\"b\", \"\", 1)\n",
    "    # Remove URL links\n",
    "    stripped = re.sub(pat2, '', souped)\n",
    "    \n",
    "    # UTF-8 BOM Decoding\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    \n",
    "    # Remove hastags and numbers\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    \n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    clean_tweets.append(tweet_cleaner1(df['Tweets'][i]))\n",
    "    \n",
    "clean_tweets = pd.DataFrame(clean_tweets)\n",
    "clean_tweets.columns = [\"Tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prajvalgupta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Tweet Length</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'no reparations no vote! no black agenda no v...</td>\n",
       "      <td>72</td>\n",
       "      <td>[vote, election, reparations, agenda, ados, bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'thoughts? san francisco ::   adam hattersley...</td>\n",
       "      <td>153</td>\n",
       "      <td>[x, hattersley, election, xa, thoughts, adamha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'@thejuanwilliams why why why are you here???...</td>\n",
       "      <td>85</td>\n",
       "      <td>[election, way, thejuanwilliams, trump]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'#berniesanders = #socialism which leads to #...</td>\n",
       "      <td>155</td>\n",
       "      <td>[x, sanders, leads, government, dependency, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'@aoc should re-think her endorsement of inde...</td>\n",
       "      <td>154</td>\n",
       "      <td>[x, sanders, ru, xe, lift, independent, republ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Tweet Length  \\\n",
       "0  b'no reparations no vote! no black agenda no v...            72   \n",
       "1  b'thoughts? san francisco ::   adam hattersley...           153   \n",
       "2  b'@thejuanwilliams why why why are you here???...            85   \n",
       "3  b'#berniesanders = #socialism which leads to #...           155   \n",
       "4  b'@aoc should re-think her endorsement of inde...           154   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [vote, election, reparations, agenda, ados, bl...  \n",
       "1  [x, hattersley, election, xa, thoughts, adamha...  \n",
       "2            [election, way, thejuanwilliams, trump]  \n",
       "3  [x, sanders, leads, government, dependency, so...  \n",
       "4  [x, sanders, ru, xe, lift, independent, republ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweets[\"Tweets\"] = clean_tweets['Tweets'].apply(remove_punctuations)\n",
    "\n",
    "# Download stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Replace the typing errors and combine politician names mentioned differently\n",
    "clean_tweets['Tweets'].replace({'realdonaldtrump':'trump', 'donaldtrump':'trump', 'donald':'trump', 'trumpxe':'trump', 'trumps':'trump', 'ntrump':'trump'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'elections':'election'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'presidential': 'president'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'democraticdebates':'demdebate','democraticdebate':'demdebate', 'demdebatemnthe ':'demdebate', 'cnndebate':'demdebate', 'demdebates':'demdebate','debates':'demdebate',' debate ': 'demdebate',}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'democratic ': 'democrats',  'thedemocrats ':'democrats', ' democrat ':'democrats', ' dem ':'democrats',}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'ewarren': 'warren',' elizabeth ': 'warren','elizabethwarren': 'warren','senwarren': 'warren' }, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'joebiden': 'biden',' joe ': 'biden'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({' bernie ': 'sanders','berniesanders': 'sanders', 'bernieyellsforus': 'sanders','stillsanders':'sanders'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'petebuttigieg': 'buttigieg',' pete ': 'buttigieg'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'andrewyang': 'yang',' andrew ': 'yang','yanggang': 'yang'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'kamalaharris': 'harris',' kamala ': 'harris'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'jobs':'job','factory':'job'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({' health ':'healthcare'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'russian':'russia'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'china':'trade'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'cyberwarfare':'russia'}, inplace=True, regex=True)\n",
    "\n",
    "# Tokenize the posts\n",
    "df['Tokens'] = clean_tweets['Tweets'].apply(word_tokenize).apply(set).apply(list)\n",
    "df['Tokens'] = df['Tokens'].apply(remove_stopwords)\n",
    "df[\"Tweets\"] = df[\"Tweets\"].apply(lambda x: x.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART B - Identifying 4 key issues mentioned in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x', 4391),\n",
       " ('xe', 4285),\n",
       " ('xa', 4160),\n",
       " ('election', 2759),\n",
       " ('trump', 1155),\n",
       " ('n', 987),\n",
       " ('president', 872),\n",
       " ('u', 835),\n",
       " ('demdebate', 785),\n",
       " ('rt', 691),\n",
       " ('warren', 450),\n",
       " ('biden', 330),\n",
       " ('new', 295),\n",
       " ('f', 290),\n",
       " ('trade', 282),\n",
       " ('via', 264),\n",
       " ('russia', 255),\n",
       " ('th', 252),\n",
       " ('war', 250),\n",
       " ('xf', 246),\n",
       " ('made', 244),\n",
       " ('democrats', 235),\n",
       " ('making', 234),\n",
       " ('months', 231),\n",
       " ('washington', 229),\n",
       " ('yang', 226),\n",
       " ('progress', 224),\n",
       " ('headlines', 221),\n",
       " ('polic', 220),\n",
       " ('tonight', 207),\n",
       " ('good', 195),\n",
       " ('report', 190),\n",
       " ('win', 189),\n",
       " ('candidates', 188),\n",
       " ('vote', 184),\n",
       " ('sanders', 180),\n",
       " ('secret', 178),\n",
       " ('people', 176),\n",
       " ('one', 175),\n",
       " ('know', 169),\n",
       " ('weapon', 169),\n",
       " ('buttigieg', 168),\n",
       " ('revealed', 168),\n",
       " ('like', 159),\n",
       " ('look', 158),\n",
       " ('cnn', 150),\n",
       " ('get', 148),\n",
       " ('think', 141),\n",
       " ('would', 141),\n",
       " ('candidate', 138),\n",
       " ('us', 137),\n",
       " ('need', 137),\n",
       " ('looks', 136),\n",
       " ('many', 129),\n",
       " ('maga', 127),\n",
       " ('amyklobuchar', 127),\n",
       " ('right', 126),\n",
       " ('years', 126),\n",
       " ('big', 125),\n",
       " ('bmcadory', 125),\n",
       " ('america', 121),\n",
       " ('xb', 121),\n",
       " ('got', 120),\n",
       " ('tech', 120),\n",
       " ('wrong', 119),\n",
       " ('certainly', 118),\n",
       " ('going', 112),\n",
       " ('harris', 111),\n",
       " ('watching', 109),\n",
       " ('security', 109),\n",
       " ('voters', 104),\n",
       " ('job', 104),\n",
       " ('potus', 99),\n",
       " ('companies', 97),\n",
       " ('plan', 96),\n",
       " ('take', 95),\n",
       " ('impeachment', 95),\n",
       " ('officials', 94),\n",
       " ('meeting', 94),\n",
       " ('time', 93),\n",
       " ('model', 93),\n",
       " ('want', 92),\n",
       " ('campaign', 91),\n",
       " ('support', 90),\n",
       " ('automation', 89),\n",
       " ('gop', 87),\n",
       " ('predicts', 87),\n",
       " ('make', 85),\n",
       " ('poll', 84),\n",
       " ('c', 82),\n",
       " ('news', 79),\n",
       " ('republican', 79),\n",
       " ('tulsigabbard', 78),\n",
       " ('every', 77),\n",
       " ('politics', 77),\n",
       " ('see', 75),\n",
       " ('still', 74),\n",
       " ('political', 73),\n",
       " ('icymi', 73),\n",
       " ('dems', 71),\n",
       " ('go', 70),\n",
       " ('back', 69),\n",
       " ('fourth', 69),\n",
       " ('great', 68),\n",
       " ('nyt', 68),\n",
       " ('voxdotcom', 68),\n",
       " ('first', 67),\n",
       " ('w', 67),\n",
       " ('media', 66),\n",
       " ('say', 66),\n",
       " ('state', 66),\n",
       " ('let', 65),\n",
       " ('nthe', 65),\n",
       " ('best', 65),\n",
       " ('mike', 65),\n",
       " ('says', 64),\n",
       " ('change', 61),\n",
       " ('even', 60),\n",
       " ('senate', 60),\n",
       " ('democratscandidates', 59),\n",
       " ('xba', 59),\n",
       " ('stage', 59),\n",
       " ('democratspresident', 59),\n",
       " ('way', 58),\n",
       " ('next', 58),\n",
       " ('watch', 58),\n",
       " ('destroying', 57),\n",
       " ('isaac', 56),\n",
       " ('e', 55),\n",
       " ('question', 54),\n",
       " ('help', 54),\n",
       " ('donated', 54),\n",
       " ('top', 53),\n",
       " ('ukraine', 53),\n",
       " ('alba', 53),\n",
       " ('davey', 53),\n",
       " ('policy', 52),\n",
       " ('please', 51),\n",
       " ('demdemdebatei', 51),\n",
       " ('lifelong', 51),\n",
       " ('love', 50),\n",
       " ('country', 50),\n",
       " ('trying', 50),\n",
       " ('said', 50),\n",
       " ('american', 50),\n",
       " ('mikemonroerm', 50),\n",
       " ('christianllamar', 50),\n",
       " ('another', 49),\n",
       " ('running', 49),\n",
       " ('foreign', 49),\n",
       " ('year', 48),\n",
       " ('ohio', 48),\n",
       " ('something', 48),\n",
       " ('tulsi', 48),\n",
       " ('cortez', 48),\n",
       " ('ocasio', 48),\n",
       " ('alexandria', 48),\n",
       " ('today', 48),\n",
       " ('vs', 48),\n",
       " ('choice', 47),\n",
       " ('really', 46),\n",
       " ('done', 46),\n",
       " ('states', 45),\n",
       " ('run', 45),\n",
       " ('xc', 45),\n",
       " ('keep', 45),\n",
       " ('voting', 45),\n",
       " ('getting', 45),\n",
       " ('race', 45),\n",
       " ('day', 44),\n",
       " ('b', 44),\n",
       " ('americans', 44),\n",
       " ('read', 44),\n",
       " ('nytimes', 44),\n",
       " ('forbes', 44),\n",
       " ('republicans', 43),\n",
       " ('important', 43),\n",
       " ('lost', 43),\n",
       " ('live', 43),\n",
       " ('open', 43),\n",
       " ('oh', 42),\n",
       " ('work', 42),\n",
       " ('talking', 42),\n",
       " ('check', 42),\n",
       " ('house', 42),\n",
       " ('r', 42),\n",
       " ('thing', 41),\n",
       " ('sure', 41),\n",
       " ('last', 41),\n",
       " ('anyone', 41),\n",
       " ('money', 41),\n",
       " ('onstage', 41),\n",
       " ('trum', 41),\n",
       " ('pay', 41),\n",
       " ('facebook', 41),\n",
       " ('memo', 41),\n",
       " ('congresswoman', 41),\n",
       " ('aoc', 40),\n",
       " ('come', 40),\n",
       " ('never', 40),\n",
       " ('must', 40),\n",
       " ('numbers', 40),\n",
       " ('find', 40),\n",
       " ('attention', 40),\n",
       " ('added', 40),\n",
       " ('party', 39),\n",
       " ('everyone', 39),\n",
       " ('left', 39),\n",
       " ('far', 39),\n",
       " ('healthcare', 39),\n",
       " ('referendum', 39),\n",
       " ('amandionair', 39),\n",
       " ('government', 38),\n",
       " ('corybooker', 38),\n",
       " ('talk', 38),\n",
       " ('dnc', 38),\n",
       " ('ban', 38),\n",
       " ('foxnews', 38),\n",
       " ('much', 37),\n",
       " ('power', 37),\n",
       " ('white', 37),\n",
       " ('well', 37),\n",
       " ('women', 37),\n",
       " ('electi', 37),\n",
       " ('tax', 37),\n",
       " ('backsanderssanders', 37),\n",
       " ('asdemocratsto', 37),\n",
       " ('real', 36),\n",
       " ('ask', 36),\n",
       " ('things', 36),\n",
       " ('join', 36),\n",
       " ('life', 36),\n",
       " ('attacks', 36),\n",
       " ('syria', 35),\n",
       " ('could', 35),\n",
       " ('gabbard', 35),\n",
       " ('betoorourke', 35),\n",
       " ('story', 35),\n",
       " ('climate', 35),\n",
       " ('nothing', 35),\n",
       " ('comes', 34),\n",
       " ('point', 34),\n",
       " ('believe', 34),\n",
       " ('video', 34),\n",
       " ('worth', 34),\n",
       " ('beto', 34),\n",
       " ('polls', 34),\n",
       " ('xef', 34),\n",
       " ('bbcworld', 34),\n",
       " ('kag', 33),\n",
       " ('night', 33),\n",
       " ('obama', 33),\n",
       " ('voter', 33),\n",
       " ('hunter', 33),\n",
       " ('show', 33),\n",
       " ('xbb', 32),\n",
       " ('needs', 32),\n",
       " ('asked', 32),\n",
       " ('impeach', 32),\n",
       " ('hope', 32),\n",
       " ('booker', 32),\n",
       " ('around', 32),\n",
       " ('medicare', 32),\n",
       " ('two', 31),\n",
       " ('issues', 31),\n",
       " ('put', 31),\n",
       " ('stop', 31),\n",
       " ('already', 31),\n",
       " ('na', 31),\n",
       " ('better', 31),\n",
       " ('primary', 31),\n",
       " ('democraticdemdebatefor', 31),\n",
       " ('shelbyfleig', 31),\n",
       " ('black', 30),\n",
       " ('chance', 30),\n",
       " ('twitter', 30),\n",
       " ('thank', 30),\n",
       " ('also', 30),\n",
       " ('analysis', 30),\n",
       " ('days', 30),\n",
       " ('pm', 30),\n",
       " ('interference', 30),\n",
       " ('week', 30),\n",
       " ('wait', 29),\n",
       " ('elected', 29),\n",
       " ('woman', 29),\n",
       " ('college', 29),\n",
       " ('end', 29),\n",
       " ('social', 29),\n",
       " ('full', 29),\n",
       " ('united', 28),\n",
       " ('part', 28),\n",
       " ('insurance', 28),\n",
       " ('yet', 28),\n",
       " ('promises', 28),\n",
       " ('p', 28),\n",
       " ('control', 28),\n",
       " ('city', 28),\n",
       " ('yes', 27),\n",
       " ('damn', 27),\n",
       " ('times', 27),\n",
       " ('tuesday', 27),\n",
       " ('thinking', 27),\n",
       " ('ba', 27),\n",
       " ('agree', 27),\n",
       " ('may', 27),\n",
       " ('looking', 27),\n",
       " ('saying', 27),\n",
       " ('waiting', 27),\n",
       " ('repadamschiff', 27),\n",
       " ('hear', 26),\n",
       " ('l', 26),\n",
       " ('anything', 26),\n",
       " ('favorite', 26),\n",
       " ('bad', 26),\n",
       " ('wants', 26),\n",
       " ('use', 26),\n",
       " ('tomorrow', 26),\n",
       " ('long', 26),\n",
       " ('answer', 26),\n",
       " ('goaxim', 26),\n",
       " ('xaf', 25),\n",
       " ('york', 25),\n",
       " ('nmy', 25),\n",
       " ('votes', 25),\n",
       " ('since', 25),\n",
       " ('early', 25),\n",
       " ('office', 25),\n",
       " ('ahead', 25),\n",
       " ('instead', 25),\n",
       " ('gun', 25),\n",
       " ('actually', 25),\n",
       " ('speakerpelosi', 25),\n",
       " ('nyou', 25),\n",
       " ('paid', 25),\n",
       " ('starts', 25),\n",
       " ('held', 24),\n",
       " ('hey', 24),\n",
       " ('fight', 24),\n",
       " ('stand', 24),\n",
       " ('someone', 24),\n",
       " ('winning', 24),\n",
       " ('away', 24),\n",
       " ('man', 24),\n",
       " ('used', 24),\n",
       " ('rights', 24),\n",
       " ('fund', 24),\n",
       " ('calling', 24),\n",
       " ('coming', 24),\n",
       " ('attack', 24),\n",
       " ('tweet', 24),\n",
       " ('influence', 24),\n",
       " ('empty', 24),\n",
       " ('thelead', 24),\n",
       " ('three', 23),\n",
       " ('democraticdemdebatewas', 23),\n",
       " ('iowa', 23),\n",
       " ('hard', 23),\n",
       " ('bernie', 23),\n",
       " ('old', 23),\n",
       " ('fact', 23),\n",
       " ('klobuchar', 23),\n",
       " ('true', 23),\n",
       " ('wow', 23),\n",
       " ('call', 23),\n",
       " ('ready', 23),\n",
       " ('nsandersn', 23),\n",
       " ('corporate', 23),\n",
       " ('billion', 23),\n",
       " ('investigation', 23),\n",
       " ('abuse', 23),\n",
       " ('legislatures', 23),\n",
       " ('million', 22),\n",
       " ('xbd', 22),\n",
       " ('front', 22),\n",
       " ('ever', 22),\n",
       " ('usa', 22),\n",
       " ('november', 22),\n",
       " ('abc', 22),\n",
       " ('rep', 22),\n",
       " ('lose', 22),\n",
       " ('free', 22),\n",
       " ('national', 22),\n",
       " ('treatment', 22),\n",
       " ('pr', 22),\n",
       " ('world', 22),\n",
       " ('ad', 22),\n",
       " ('lindseygrahamsc', 22),\n",
       " ('key', 22),\n",
       " ('carolina', 22),\n",
       " ('tag', 22),\n",
       " ('lead', 22),\n",
       " ('foxandfriends', 22),\n",
       " ('councils', 22),\n",
       " ('dozens', 22),\n",
       " ('updates', 22),\n",
       " ('xaa', 21),\n",
       " ('idea', 21),\n",
       " ('feel', 21),\n",
       " ('democracy', 21),\n",
       " ('everything', 21),\n",
       " ('questions', 21),\n",
       " ('place', 21),\n",
       " ('beat', 21),\n",
       " ('october', 21),\n",
       " ('hillary', 21),\n",
       " ('sen', 21),\n",
       " ('corruption', 21),\n",
       " ('name', 21),\n",
       " ('models', 21),\n",
       " ('taking', 21),\n",
       " ('gets', 21),\n",
       " ('hillaryclinton', 21),\n",
       " ('google', 21),\n",
       " ('shows', 21),\n",
       " ('et', 21),\n",
       " ('giving', 21),\n",
       " ('post', 21),\n",
       " ('runforsomething', 21),\n",
       " ('illness', 21),\n",
       " ('price', 21),\n",
       " ('substance', 21),\n",
       " ('mental', 21),\n",
       " ('bringingreceipts', 21),\n",
       " ('public', 20),\n",
       " ('biggest', 20),\n",
       " ('face', 20),\n",
       " ('gon', 20),\n",
       " ('ni', 20),\n",
       " ('single', 20),\n",
       " ('turnout', 20),\n",
       " ('presidency', 20),\n",
       " ('h', 20),\n",
       " ('lot', 20),\n",
       " ('purchase', 20),\n",
       " ('case', 20),\n",
       " ('nthis', 20),\n",
       " ('shirt', 20),\n",
       " ('lookin', 20),\n",
       " ('impeaching', 19),\n",
       " ('give', 19),\n",
       " ('person', 19),\n",
       " ('tell', 19),\n",
       " ('start', 19),\n",
       " ('moody', 19),\n",
       " ('clear', 19),\n",
       " ('game', 19),\n",
       " ('juliancastro', 19),\n",
       " ('seriously', 19),\n",
       " ('latest', 19),\n",
       " ('meet', 19),\n",
       " ('maybe', 19),\n",
       " ('general', 19),\n",
       " ('criticized', 19),\n",
       " ('thomaskaine', 19),\n",
       " ('clue', 19),\n",
       " ('presidentdemdebatelive', 19),\n",
       " ('product', 19),\n",
       " ('agilenthawking', 19),\n",
       " ('pric', 19),\n",
       " ('discounts', 19),\n",
       " ('nsale', 19),\n",
       " ('ticket', 18),\n",
       " ('sensanders', 18),\n",
       " ('truth', 18),\n",
       " ('remember', 18),\n",
       " ('nomination', 18),\n",
       " ('deal', 18),\n",
       " ('issue', 18),\n",
       " ('across', 18),\n",
       " ('breaking', 18),\n",
       " ('seems', 18),\n",
       " ('reason', 18),\n",
       " ('thought', 18),\n",
       " ('pre', 18),\n",
       " ('enough', 18),\n",
       " ('k', 18),\n",
       " ('might', 18),\n",
       " ('supporters', 18),\n",
       " ('debate', 18),\n",
       " ('listen', 18),\n",
       " ('follow', 18),\n",
       " ('cbsnews', 18),\n",
       " ('bloomberg', 18),\n",
       " ('msnbc', 18),\n",
       " ('north', 18),\n",
       " ('care', 17),\n",
       " ('history', 17),\n",
       " ('matter', 17),\n",
       " ('happen', 17),\n",
       " ('vice', 17),\n",
       " ('administration', 17),\n",
       " ('took', 17),\n",
       " ('exactly', 17),\n",
       " ('continue', 17),\n",
       " ('exposecnn', 17),\n",
       " ('able', 17),\n",
       " ('tomsteyer', 17),\n",
       " ('moment', 17),\n",
       " ('putin', 17)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = []\n",
    "# Count the frequency of the words\n",
    "for i in range(len(df[\"Tokens\"])):\n",
    "    count += df.iloc[i]['Tokens']\n",
    "count = [x.lower() for x in count]\n",
    "word_freq = nltk.FreqDist(count)\n",
    "\n",
    "# 500 most frequent words\n",
    "top_words = word_freq.most_common(500)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Top Republican candidate - Donald Trump <br>\n",
    "Top Democratic candidate - Elizabeth Warren <br><br>\n",
    "Top 5 issues found:\n",
    "1. Trade\n",
    "2. Russia\n",
    "3. Progress\n",
    "4. Security\n",
    "5. Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART C.\tPerform lift and sentiment analysis on a candidate and an issue:\n",
    "To isolate a candidate and an issue, you have to write a parser that selects tweets that mention a candidate, takes a window around an issue, and chops off everything else, as shown in class.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates we are exploring: \n",
      " [('trump', 1155), ('warren', 450)]\n"
     ]
    }
   ],
   "source": [
    "top_candidiate_count = []\n",
    "\n",
    "unique_candidates = ['trump','warren']\n",
    "\n",
    "for (key, items) in top_words:\n",
    "    if key in unique_candidates:\n",
    "        candidate_count = (key,items)\n",
    "        top_candidiate_count.append(candidate_count)  \n",
    "\n",
    "print ('Candidates we are exploring: \\n' , top_candidiate_count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def issue_attri_count(issueList, df_a, attributeList):\n",
    "    comb_count=[]\n",
    "    car_count={}\n",
    "    attrib_count={}\n",
    "    for car in issueList:\n",
    "        num_car=0\n",
    "        for comment in df_a.Tweets:\n",
    "                if car in comment:\n",
    "                    num_car+=1\n",
    "        car_count[car]=num_car\n",
    "        for attrib in attributeList:\n",
    "            num_combined=0\n",
    "            num_attrib=0\n",
    "            for comment in df_a.Tweets:\n",
    "                if car in comment and attrib in comment:\n",
    "                    num_combined+=1\n",
    "                if attrib in comment:\n",
    "                    num_attrib+=1\n",
    "            attrib_count[attrib]=num_attrib\n",
    "            comb_count.append(((car,attrib),num_combined))\n",
    "    return [[car_count,attrib_count,comb_count]]\n",
    "\n",
    "def calculate_lift(car, attrib, combined,attrib_count,car_count):\n",
    "    if(attrib_count[attrib]==0):\n",
    "        print(attrib)\n",
    "    if(car_count[car]==0):\n",
    "        print(car)\n",
    "    lift = (4655*combined)/(car_count[car]*attrib_count[attrib])\n",
    "    return lift\n",
    "\n",
    "# Function to check if a issue is mentioned in a post or not\n",
    "def issue_count(item):\n",
    "    if issue in item:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributeList = ['trump','warren','trade','russia','progress','security','job']\n",
    "top_issues_count = []\n",
    "for i,j in top_words:\n",
    "    if i in attributeList:\n",
    "        top_issues_count.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Frequency count of the top 5 issues are: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('trump', 1155),\n",
       " ('warren', 450),\n",
       " ('trade', 282),\n",
       " ('russia', 255),\n",
       " ('progress', 224),\n",
       " ('security', 109),\n",
       " ('job', 104)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The Frequency count of the top 5 issues are: \\n\")\n",
    "top_issues_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lift Matrix:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trump</th>\n",
       "      <th>warren</th>\n",
       "      <th>trade</th>\n",
       "      <th>russia</th>\n",
       "      <th>progress</th>\n",
       "      <th>security</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.668548</td>\n",
       "      <td>0.312692</td>\n",
       "      <td>0.325460</td>\n",
       "      <td>0.046313</td>\n",
       "      <td>0.190349</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warren</th>\n",
       "      <td>0.668548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.094421</td>\n",
       "      <td>0.052209</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.122141</td>\n",
       "      <td>7.680769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trade</th>\n",
       "      <td>0.312692</td>\n",
       "      <td>0.094421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083312</td>\n",
       "      <td>20.865312</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.204276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>russia</th>\n",
       "      <td>0.325460</td>\n",
       "      <td>0.052209</td>\n",
       "      <td>0.083312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.431085</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>progress</th>\n",
       "      <td>0.046313</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>20.865312</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>0.190349</td>\n",
       "      <td>0.122141</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.431085</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.528493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job</th>\n",
       "      <td>0.199500</td>\n",
       "      <td>7.680769</td>\n",
       "      <td>0.204276</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.528493</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             trump    warren      trade    russia   progress  security  \\\n",
       "trump          NaN  0.668548   0.312692  0.325460   0.046313  0.190349   \n",
       "warren    0.668548       NaN   0.094421  0.052209   0.010000  0.122141   \n",
       "trade     0.312692  0.094421        NaN  0.083312  20.865312  0.010000   \n",
       "russia    0.325460  0.052209   0.083312       NaN   0.010000  0.431085   \n",
       "progress  0.046313  0.010000  20.865312  0.010000        NaN  0.010000   \n",
       "security  0.190349  0.122141   0.010000  0.431085   0.010000       NaN   \n",
       "job       0.199500  7.680769   0.204276  0.010000   0.010000  0.528493   \n",
       "\n",
       "               job  \n",
       "trump     0.199500  \n",
       "warren    7.680769  \n",
       "trade     0.204276  \n",
       "russia    0.010000  \n",
       "progress  0.010000  \n",
       "security  0.528493  \n",
       "job            NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_issues =[]\n",
    "for issue, count in top_issues_count:\n",
    "    top_issues.append(issue) \n",
    "\n",
    "issue_df = pd.DataFrame(columns = top_issues)\n",
    "\n",
    "for issue in top_issues:\n",
    "    issue_df[issue] = df['Tokens'].apply(issue_count)        \n",
    "\n",
    "lift_df = pd.DataFrame(columns = top_issues)\n",
    "\n",
    "for i in range(len(top_issues)):\n",
    "    new_list = []\n",
    "    for j in range(len(top_issues)):\n",
    "        if (i!=j):\n",
    "            num = ((issue_df[top_issues[i]] + issue_df[top_issues[j]]) > 1).sum()\n",
    "            dem = issue_df[top_issues[j]].sum()*issue_df[top_issues[i]].sum()\n",
    "            lift = num * len(issue_df) / dem\n",
    "            lift_df.loc[top_issues[i],top_issues[j]] = lift\n",
    "\n",
    "lift_df = lift_df.replace(0,0.01)\n",
    "print ('Lift Matrix:\\n')\n",
    "lift_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/prajvalgupta/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_list = lift_df[(lift_df.index == \"trump\") | (lift_df.index == \"warren\")].stack().index.tolist()\n",
    "sorted_lifts = []\n",
    "for index,column in pair_list:\n",
    "    sorted_lifts.append(((index,column),lift_df[index][column]))\n",
    "sorted_lifts.pop(0)\n",
    "sorted_lifts.pop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate  Issue  Sentiment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('trump', 'trade'): 0.05,\n",
       " ('trump', 'russia'): 0.03,\n",
       " ('trump', 'progress'): 0.33,\n",
       " ('trump', 'security'): 0.25,\n",
       " ('trump', 'job'): 0.08,\n",
       " ('warren', 'trade'): 0.15,\n",
       " ('warren', 'russia'): 0.0,\n",
       " ('warren', 'progress'): -0.2,\n",
       " ('warren', 'security'): 0.3,\n",
       " ('warren', 'job'): -0.53}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_lifts_nt = []\n",
    "sentiment_dict = {}\n",
    "    \n",
    "def sentiment_value(tweet):\n",
    "    analyser = sentiment()\n",
    "    result = analyser.polarity_scores(tweet)\n",
    "    score = result['compound']\n",
    "    return round(score,1)\n",
    "count=0\n",
    "for i in sorted_lifts:\n",
    "    candidate = i[0][0]\n",
    "    place = i[0][1]\n",
    "    key = (candidate,place)\n",
    "    sentiments = []\n",
    "    for tweet in clean_tweets['Tweets']:\n",
    "        if candidate in tweet and place in tweet:\n",
    "            words = re.findall(r'\\w+', tweet)\n",
    "            if place in words:\n",
    "                index = words.index(place)\n",
    "                left = words[index - 3:index]\n",
    "                right = words[index + 1:index + 4]\n",
    "                tweetyy = \" \".join(left) + \" \" + words[index] + \" \" + \" \".join(right)\n",
    "            else:\n",
    "                tweetyy = tweet\n",
    "            sv = sentiment_value(tweetyy)\n",
    "            sentiments.append(sv)\n",
    "#     print(sentiments)\n",
    "    avg_sentiment = round(sum(sentiments)/len(sentiments),2)\n",
    "    sentiment_dict[key] = avg_sentiment\n",
    "\n",
    "print(\"Candidate  Issue  Sentiment\")\n",
    "sentiment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Lift and Sentiment for Candidate-Issue.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART D - MDS Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Lift Matrix:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trump</th>\n",
       "      <th>warren</th>\n",
       "      <th>trade</th>\n",
       "      <th>russia</th>\n",
       "      <th>progress</th>\n",
       "      <th>security</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.495778</td>\n",
       "      <td>3.198032</td>\n",
       "      <td>3.072578</td>\n",
       "      <td>21.592389</td>\n",
       "      <td>5.253505</td>\n",
       "      <td>5.012519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warren</th>\n",
       "      <td>1.495778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.590886</td>\n",
       "      <td>19.153731</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>8.187281</td>\n",
       "      <td>0.130195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trade</th>\n",
       "      <td>3.198032</td>\n",
       "      <td>10.590886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.003005</td>\n",
       "      <td>0.047926</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>4.895343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>russia</th>\n",
       "      <td>3.072578</td>\n",
       "      <td>19.153731</td>\n",
       "      <td>12.003005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.319730</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>progress</th>\n",
       "      <td>21.592389</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.047926</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>5.253505</td>\n",
       "      <td>8.187281</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.319730</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.892172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job</th>\n",
       "      <td>5.012519</td>\n",
       "      <td>0.130195</td>\n",
       "      <td>4.895343</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.892172</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              trump      warren       trade      russia    progress  \\\n",
       "trump      0.000000    1.495778    3.198032    3.072578   21.592389   \n",
       "warren     1.495778    0.000000   10.590886   19.153731  100.000000   \n",
       "trade      3.198032   10.590886    0.000000   12.003005    0.047926   \n",
       "russia     3.072578   19.153731   12.003005    0.000000  100.000000   \n",
       "progress  21.592389  100.000000    0.047926  100.000000    0.000000   \n",
       "security   5.253505    8.187281  100.000000    2.319730  100.000000   \n",
       "job        5.012519    0.130195    4.895343  100.000000  100.000000   \n",
       "\n",
       "            security         job  \n",
       "trump       5.253505    5.012519  \n",
       "warren      8.187281    0.130195  \n",
       "trade     100.000000    4.895343  \n",
       "russia      2.319730  100.000000  \n",
       "progress  100.000000  100.000000  \n",
       "security    0.000000    1.892172  \n",
       "job         1.892172    0.000000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the dissimilarity matrix which is the input for plotting MDS plot\n",
    "\n",
    "lift_inverse_df = 1/lift_df\n",
    "np.fill_diagonal(lift_inverse_df.values, 0)\n",
    "\n",
    "print ('Inverse Lift Matrix:\\n')\n",
    "lift_inverse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = np.random.RandomState(seed=3)\n",
    "\n",
    "mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=seed,\n",
    "                   dissimilarity=\"precomputed\", n_jobs=1)\n",
    "pos = mds.fit(lift_inverse_df)\n",
    "coords = pos.embedding_\n",
    "\n",
    "\n",
    "plt.subplots_adjust(bottom = 0.1)\n",
    "plt.scatter(\n",
    "    coords[:, 0], coords[:, 1], marker = 'o'\n",
    "    )\n",
    "for label, x, y in zip(top_issues, coords[:, 0], coords[:, 1]):\n",
    "    \n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy = (x, y), xytext = (-5, 5),\n",
    "        textcoords = 'offset points', ha = 'right', va = 'bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK E - Lift and Sentiment Analysis for the canditates in three states - Michigan, Pennsylvania and Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The sorted Lift scores for three states are: \n",
      "\n",
      "(('warren', 'battleground3'), 1.5017094017094017)\n",
      "(('trump', 'battleground3'), 1.1441595441595442)\n",
      "(('warren', 'battleground2'), 1.1067019400352733)\n",
      "(('trump', 'battleground2'), 1.0513668430335097)\n",
      "(('trump', 'battleground1'), 0.8793793793793794)\n",
      "(('warren', 'battleground1'), 0.6783783783783783)\n",
      "{'warrenbattleground3': [-0.3, -0.5, 0.4, 0.3], 'trumpbattleground3': [0.0, -0.1, 0.0, 0.0, 0.0, 0.2, 0.6, 0.4, 0.4], 'warrenbattleground2': [0.6, 0.3, 0.0, 0.0, -0.5, 0.0, 0.4, 0.4, 0.4], 'trumpbattleground2': [0.0, -0.3, 0.5, 0.3, 0.0, -0.4, 0.0, 0.7, 1.0, 0.0, -0.5, 0.0, 0.0, 0.0, 0.2, 0.2, 0.6, 0.2, 0.2, -0.3, 0.0, 0.2, -0.3, -0.2, 0.2, -0.4], 'trumpbattleground1': [0.4, 0.6, 0.2, -0.4, 0.2, 0.6, 0.2, 0.2, 0.6, 0.4, 0.0, -0.4], 'warrenbattleground1': [0.4, 0.0]}\n",
      "\n",
      "\n",
      "The Sentiment scores for three states are: \n",
      "\n",
      "{'warrenbattleground3': -0.03, 'trumpbattleground3': 0.17, 'warrenbattleground2': 0.18, 'trumpbattleground2': 0.07, 'trumpbattleground1': 0.22, 'warrenbattleground1': 0.2}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets.csv\")\n",
    "# Remove newline from each post & drop rows with null values\n",
    "df = df.replace('\\n','', regex=True)\n",
    "df = df.dropna().reset_index()\n",
    "df.drop('index', axis=1, inplace=True)\n",
    "df.head(5)\n",
    "# Replacing State names with 'battleground' \n",
    "# Adding battlegrund number word inside tweet\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i,'location']:\n",
    "        if 'MI' in df.loc[i,'location'].upper() or 'MICHIGAN' in df.loc[i,'location'].split(',') or 'MICHIGAN' in [x.strip() for x in df.loc[i,'location'].split(',')]:\n",
    "            df.loc[i,'location']='battleground1'\n",
    "            df.loc[i,'text']+=' '+'battleground1'+' '\n",
    "        if 'PA' in df.loc[i,'location'].upper() or 'PENNSYLVANIA' in df.loc[i,'location'].split(',') or 'PENNSYLVANIA' in [x.strip() for x in df.loc[i,'location'].split(',')]:\n",
    "            df.loc[i,'location']='battleground2'\n",
    "            df.loc[i,'text']+=' '+'battleground2'+' '\n",
    "        if 'WI' in df.loc[i,'location'].upper() or 'WISCONSIN' in df.loc[i,'location'].split(',') or 'WISCONSIN' in [x.strip() for x in df.loc[i,'location'].split(',')]:\n",
    "            df.loc[i,'location']='battleground3'\n",
    "            df.loc[i,'text']+=' '+'battleground3'+' '\n",
    "df.tail(20)\n",
    "df=df.rename(columns = {'text':'Tweets'})\n",
    "def cand_state_count(brandList, df_a, attributeList):\n",
    "    comb_count=[]\n",
    "    car_count={}\n",
    "    attrib_count={}\n",
    "    for car in brandList:\n",
    "        num_car=0\n",
    "        for comment in df_a.Tweets:\n",
    "              if car in comment.lower():\n",
    "                    num_car+=1\n",
    "        car_count[car]=num_car\n",
    "        for attrib in attributeList:\n",
    "            num_combined=0\n",
    "            num_attrib=0\n",
    "            for comment in df_a.Tweets:\n",
    "                if car in comment.lower() and attrib in comment.lower():\n",
    "                    num_combined+=1\n",
    "                if attrib in comment.lower():\n",
    "                    num_attrib+=1\n",
    "            attrib_count[attrib]=num_attrib\n",
    "            comb_count.append(((car,attrib),num_combined))\n",
    "    return [[car_count,attrib_count,comb_count]]\n",
    "def calculate_lift(car, attrib, combined, attrib_count, car_count):\n",
    "    if(attrib_count[attrib]==0):\n",
    "        print(attrib)\n",
    "    if(car_count[car]==0):\n",
    "        print(car)\n",
    "    lift = (502*combined)/(car_count[car]*attrib_count[attrib])\n",
    "    return lift\n",
    "# Lifts of candidates with each state\n",
    "candList = ['trump','warren']\n",
    "statesList = ['battleground1','battleground2','battleground3']\n",
    "# Lift scores for top 5 brands\n",
    "lift_scores={}\n",
    "df_a=df[df.location.str.startswith('battleground')]\n",
    "for i in cand_state_count(candList, df_a, statesList):\n",
    "    for k,num in i[2]:\n",
    "        lift_scores[(k[0],k[1])] = calculate_lift(k[0],k[1], num,i[1],i[0])\n",
    "sorted_lifts = sorted(lift_scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(\"\\n\\nThe sorted Lift scores for three states are: \\n\")\n",
    "for i in sorted_lifts:\n",
    "    print(i)\n",
    "sorted_lifts_nt = []\n",
    "Sentiment_dict = {}\n",
    "for i in sorted_lifts:\n",
    "    tuple_ = i[0]\n",
    "    sorted_lifts_nt.append(tuple_)\n",
    "    Sentiment_dict[tuple_[0]+tuple_[1]]=[]\n",
    "def sentiment_value(tweet):\n",
    "    analyser = sentiment()\n",
    "    result = analyser.polarity_scores(tweet)\n",
    "    score = result['compound']\n",
    "    return round(score,1)\n",
    "count=0\n",
    "time_line=[]\n",
    "Sentiment_list = []\n",
    "Key_list =[]\n",
    "for i in sorted_lifts_nt:\n",
    "    candidate = i[0]\n",
    "    place = i[1]\n",
    "    count += 1\n",
    "    key = candidate + place\n",
    "    key_space = candidate + ' ' +place\n",
    "    Key_list.append(key_space)\n",
    "    for tweet in df['Tweets']:\n",
    "        if candidate in [x.strip().lower() for x in tweet.split(' ')] and place in [x.strip().lower() for x in tweet.split(' ')]:\n",
    "            sv = sentiment_value(tweet)\n",
    "            temp_list = []\n",
    "            temp_list.append(key)\n",
    "            temp_list.append(sv)\n",
    "            Sentiment_dict[key].append(sv)\n",
    "            Sentiment_list.append(temp_list)\n",
    "            \n",
    "print(Sentiment_dict)\n",
    "Sentiment_states={}\n",
    "for i in Sentiment_dict.keys():\n",
    "    Sentiment_states[i]=round(np.array(Sentiment_dict[i]).mean(),2)\n",
    "\n",
    "print(\"\\n\\nThe Sentiment scores for three states are: \\n\")\n",
    "print(Sentiment_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Lif and Sentiment for Candidate - Battleground.png\" width=\"800\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK F - What advice would you give to each of the two candidates above based on your analyses in C, D and E above?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Lift Analysis above, both candidates (Trump and Warren) face the common goal of taking a clear lead in the political landscape. Our advice to both candidates is simple; you must break the deadlock and take the clear lead. Although this is easier said than done, we believe that the ultimate goal will be best assisted from gathering massive amounts of peoples real opinions on topics quickly. Twitter is the best forum for us to get such feedback while having libraries that enable us to scrape tweets from twitter users effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRUMP \n",
    "\n",
    "Trump has two major points based on our analysis from Twitter.\n",
    "\n",
    "-\tTrumps sentiment for Progress is high 0.33 while having a moderate lift value of 0.04.\n",
    "-\tTrumps Russia sentiment is at 0.03, but his lift value is moderately high for our analysis at 0.32.\n",
    "\n",
    "First of all, Trump has done a successful job in progressing with the China Trade Deal, according to the Twittersphere. However, very few people are talking about the deal.<br>\n",
    "\n",
    "-\tThe U.S. China trade war has been making headlines for months now. Some progress has been made but, Washington Policy Analyst Ed Mills doesnt foresee a broad resolution being reached in advance of the 2020 election.<br>\n",
    "\n",
    "This is why Trump has a high sentiment score with Progress. This result was the most surprising out of all the analysis we have done in our data set. The problem is no one seems to be talking about the major progress.<br>\n",
    "\n",
    "Next, Trump is highly associated with Russia. This isnt surprising, because this has been a major talking point for people on the other side of the aisle. However, what is surprising is the sentiment score is rather neutral, especially when you consider that there are more active users who have viewpoints more aligned with the democratic party. An example of a tweet that reduces the negative sentiment in his tweet.<br>\n",
    "-\tb'Dedicated to President Donald Trump #TrumpRussia, #USA, #2020Election, #Democracy, https://t.co/ejzysSBKv4'<br>\n",
    "\n",
    "Overall, we recommend Trump to revise his policies on Russia/Putin because both sides of the aisle dont view like his policies, and Russia is frequently associated with Trump. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARREN\n",
    "\n",
    "Warren is also struggling on the front of jobs and the trade war:\n",
    "\n",
    "-\tWarrens sentiment for jobs is very low at -0.53, but the lift value is high at 7.68\n",
    "-\tMeanwhile, Warren has to improve her policy in trade because her campaign has a low sentiment value 0.15 and a lift score of 0.09.\n",
    "\n",
    "Warrens campaign is excelling on exposure and connection with jobs (she has a very high lift value of 7.68). However, she is not capitalizing to her fullest potential, as her sentiment is very low. She needs to convince America that she is a candidate who will be good for American jobs. We believe Warrens low sentiment value is due to one tweet.<br>\n",
    "\n",
    "-\tWarren is right, automation is not destroying as many factory jobs as people think \n",
    "\n",
    "Looking at all of our tweets that have Warren and Jobs, this tweet was 90% of results for that query. However, we still suggest that Warren layout to America her plans to create and maintain jobs. Any sort of effort to highlight this will be extremely effective as her lift is high. Thus, the campaign should spare no expense in advertising her stance on jobs to increase sentiment. Lastly, unusual about this sentiment score is we believe destroying is strongly pulling the sentiment below, despite Warren being right.<br>\n",
    "\n",
    "In addition, Warren needs to adjust her stance on the Trade War. The trade war is among the top issues for the United States, and Americans remain unconvinced that Warren is the answer. With this in mind, we suggest at the next debate that Warren takes a firm stance (preferably the opposite of Trumps as his sentiment is negative). By taking a strong stance, Warren will include her name in the trade war narrative, and thus will increase her lift value of 0.09. Finally, she needs to improve her sentiment value with trade. Our suggestion is to advertise Warrens experience with economics and trade. If she can highlight why she is the right woman for the job, the sentiment is sure to follow. <br>\n",
    "\n",
    "Tweet example:\n",
    "1. Warren & Trade Even I dont buy #ElizabethWarrens claim that bad #trade policies have been Americas main #jobs problem lately.\\xe2\\x80\\xa6 https://t.co/AL4TFaDP47\n",
    "\n",
    "\n",
    "\n",
    "Overall, we recommend taking all of our recommendations with a grain of salt, because nearly all of our lift values are below one except Elizabeth Warren and Jobs Lift Value. In addition, the only data point with a high lift value appears to be somewhat biased.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BATTLEGROUND ADVICE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were advising either candidate, we would recommend gathering more data over a greater period to gain a data set with less variance. This improves the authenticity of your lift scores and sentiment values, especially when targeting specific states, which significantly reduces the relevant data compared to obtaining values from data across the United States. All of these states are known to have a vast working class who isnt on social media, especially when compared to states like New York, California, or Florida. <br>\n",
    "\n",
    "Lift scores often show candidates are associated with a topic, and how relevant are they to the discussion. For calculating the Lift Values with Candidates and battleground states, we were topic agnostic and calculated the lift value by candidate names in tweet and location of the tweet. In general, a higher lift value is positive. Lastly, if the lift value is below 1, we recommend both are candidates to take the sentiment value with a large grain of salt.<br>\n",
    "\n",
    "### TRUMP \n",
    "\n",
    "#### Lift\n",
    "Overall, all of the battleground states dont have significant lift scores that are associated with Donald Trump. (Significance defines as a value over 1). However, Warren is talked about much more frequently than Donald Trump in all three battleground states. This contrasts President Trumps presidential run in 2016 against Hillary Clinton. However, since we collected tweets from the last week on October 15th, so this is the best representation, due to the limited timeframe of collection. However, when Trump needs to start campaigning against Warren directly, we recommend that he should push more advertisements into those states.<br>\n",
    "\n",
    "#### Sentiment\n",
    "Overall, President Trump and Elizabeth Warren have very comparable Sentiment values within the battleground state. Both have a slightly positive sentiment. However, Trump has marginally better sentiment over Elizabeth Warren. However, there is still a lot of room for improvement. We recommend that Donald Trump replicate his success in the battleground states by continue promising for more industrial jobs/business for middle-class workers. <br>\n",
    "\n",
    "### WARREN\n",
    "\n",
    "#### Lift\n",
    "Warren is doing an excellent job of being relevant to voters in the battleground states. Her lift values are incredibly high in Pennsylvania and Wisconsin, with a mediocre Michigan lift value. We find this very surprising because she hasnt had campaign rallies or campaign events in Michigan, Pennsylvania, or Wisconsin in the last month.<br>\n",
    "\n",
    "We believe all of this attention is coming from the 4th Democratic Primary debates, as per these tweets.\n",
    "-\tb'RT @voxdotcom: Warren is right, automation is not destroying as many factory jobs as people think #DemDebate https://t.co/QYKoSNVJHN' (Philadelphia)\n",
    "-\t\"A sign of @EWarren's status as front-runner: midtier candidates like @PeteButtigieg and @AmyKlobuchar are attacking\\xe2\\x80\\xa6 https://t.co/2f3584cZVi\" (Flint, MI)\n",
    "\n",
    "#### Sentiment\n",
    "\n",
    "Overall, Warren has a slightly positive sentiment value and an effective neutral sentiment value in Wisconsin. Its good that she doesnt have a negative sentiment. However, she will likely need to have a greater sentiment value in the states than Trump, because Democrats tend to tweet far more frequently than Republicans. Also, I would assume\n",
    "Luckily for Elizabeth Warren, she has a significant gap in her positive sentiment value between her and Trump in Pennsylvania, which is one of the largest states by electoral college count.<br>\n",
    "\n",
    "#### Reference\n",
    "The electoral college votes for the 2020 election in the battleground states are Wisconsin (10), Pennsylvania (20), and Michigan (16).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
